{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Parsing, cleaning, and storing PDFs </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how I parsed the PDFs of the CRS reports, did some preliminary cleaning, and then put them into a MongoDB collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opening up a connection to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client = MongoClient('mongodb://localhost:27017/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db = client.project_4_database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the locations of all of the PDFs from the CRS reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pdf_locs = ['/Users/jonathanjramirez/Documents/metis_project_4/fas.org/sgp/crs/*.pdf',\n",
    "            \"/Users/jonathanjramirez/Documents/metis_project_4/fas.org/sgp/crs/terror/*.pdf\",\n",
    "            \"/Users/jonathanjramirez/Documents/metis_project_4/fas.org/sgp/crs/homesec/*.pdf\",\n",
    "            \"/Users/jonathanjramirez/Documents/metis_project_4/fas.org/sgp/crs/intel/*.pdf\",\n",
    "            \"/Users/jonathanjramirez/Documents/metis_project_4/fas.org/sgp/crs/mideast/*.pdf\",\n",
    "            \"/Users/jonathanjramirez/Documents/metis_project_4/fas.org/sgp/crs/misc/*.pdf\",\n",
    "            \"/Users/jonathanjramirez/Documents/metis_project_4/fas.org/sgp/crs/row/*.pdf\",\n",
    "            \"/Users/jonathanjramirez/Documents/metis_project_4/fas.org/sgp/crs/secrecy/*.pdf\",\n",
    "            \"/Users/jonathanjramirez/Documents/metis_project_4/fas.org/sgp/crs/space/*.pdf\",\n",
    "            \"/Users/jonathanjramirez/Documents/metis_project_4/fas.org/sgp/crs/weapons/*.pdf\",\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topics = ['crs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code will give the file paths to all of the PDFs in the folders above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "total_pdf_file_paths = []\n",
    "\n",
    "for loc in pdf_locs[1:]:\n",
    "    \n",
    "    pdfs_paths = glob.glob(loc)\n",
    "    \n",
    "    for path in pdfs_paths:\n",
    "\n",
    "        if ('R' in path):\n",
    "            total_pdf_file_paths.append(path)\n",
    "    topics.append(loc[loc.index('crs') + len('crs/'):loc.index('/*')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the URLs for later Flask App use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "urls = [file[50:] for file in total_pdf_file_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"urls.pkl\", \"wb\") as output_file:\n",
    "    pickle.dump(urls, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to find the file names (all starting with R)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_names = []\n",
    "for path in total_pdf_file_paths:\n",
    "    file_names.append(re.search(r'R[\\w]*',path)[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Textract is an awesome PDF parsing library that will make data collection a lot easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import textract\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class crs_parser():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.articles = []\n",
    "        self.titles = []\n",
    "        \n",
    "    def convert_to_string(self,pdfs):\n",
    "        \n",
    "        while pdfs:\n",
    "            \n",
    "            try:\n",
    "                pdf = pdfs.pop()\n",
    "\n",
    "                #print('Processing {}'.format(pdf))\n",
    "                #print('{} documents left'.format(len(pdfs)))\n",
    "\n",
    "                text = (textract.\n",
    "                        process(pdf).\n",
    "                        decode('utf-8').\n",
    "                        replace('\\n', ' ').\n",
    "                        replace('.htm', ' ').\n",
    "                        replace('.gov', ' '))\n",
    "\n",
    "                text = ''.join(x for x in text if x in string.printable)\n",
    "                text = ''.join([i for i in text if not i.isdigit()])\n",
    "\n",
    "                self.articles.append(text)\n",
    "\n",
    "                name = re.search(r'R[\\w]*',pdf)[0]\n",
    "\n",
    "                #print(name)\n",
    "\n",
    "                self.titles.append(name)\n",
    "                \n",
    "            except:\n",
    "                \n",
    "                pass\n",
    "\n",
    "            \n",
    "    def article_tups(self):\n",
    "        \n",
    "        self.art_tups = []\n",
    "        \n",
    "        for i,article in enumerate(self.articles):\n",
    "            \n",
    "            self.art_tups.append((self.titles[i], article))\n",
    "        #print(self.art_tups)\n",
    "            \n",
    "    def load_articles(self,articles):\n",
    "                \n",
    "        self.convert_to_string(articles)\n",
    "        \n",
    "        self.article_tups()\n",
    "        \n",
    "        \n",
    "    def to_mongo(self, db_client, collection):\n",
    "\n",
    "        for i,pdf in enumerate(self.articles):\n",
    "\n",
    "            db_client.collection.insert_one({self.titles[i]:pdf})    \n",
    "        \n",
    "    def get_art_tups(self):\n",
    "        \n",
    "        return self.art_tups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crs = crs_parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "crs.load_articles(total_pdf_file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Investigate duplicate texts </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, let's just do our due diligence and make sure that there weren't any duplicate texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dict = {}\n",
    "\n",
    "for i,article in enumerate(crs.articles):\n",
    "    \n",
    "    title = crs.titles[i]\n",
    "    data_dict[title] = {'name': title, 'text': article, 'url': urls[i]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "crs_df = pd.DataFrame(pd.DataFrame.from_dict(list(data_dict[crs.titles[0]].items())).T.iloc[1,:]).T\n",
    "crs_df.columns = ['name', 'text', 'url']\n",
    "\n",
    "for datum in crs.titles[1:]:\n",
    "    \n",
    "    new_row = pd.DataFrame(pd.DataFrame.from_dict(list(data_dict[datum].items())).T.iloc[1,:]).T\n",
    "    new_row.columns = ['name', 'text', 'url']\n",
    "    crs_df = crs_df.append(new_row,ignore_index= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_txts = np.array(crs_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_uniques(df):\n",
    "    \n",
    "    unique_ids = []\n",
    "\n",
    "    for text in df['text'].unique():\n",
    "\n",
    "        unique_ids.append(list(df[df['text'] == text]['name'])[0])\n",
    "            \n",
    "    return unique_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uniques = find_uniques(crs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"uniques.pkl\", \"wb\") as output_file:\n",
    "    pickle.dump(uniques, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pdf in uniques:\n",
    "\n",
    "     db.raw_pdfs.insert_one({pdf:data_dict[pdf]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"crs_pdfs.pkl\", \"wb\") as output_file:\n",
    "    pickle.dump(crs, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Cleaning and Storing</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we do some basic cleaning such as removing stop words, symbols, and multiple ellipses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "crs = pickle.load(open('crs_pdfs.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathanjramirez/anaconda/envs/py3.6/lib/python3.6/site-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.22) or chardet (2.3.0) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class cleaner():\n",
    "    \n",
    "    def __init__(self, articles, user_det_stopwords = []):\n",
    "        \n",
    "        self.articles = articles\n",
    "        self.abbreviations = []\n",
    "        self.user_det_stopwords = user_det_stopwords\n",
    "        \n",
    "    def doc_cleaner(self, doc):\n",
    "\n",
    "        alt_stop_words = self.user_det_stopwords + ['republican', 'Republican', 'democrat', 'Democrat'] \n",
    "        cleaned = (doc.\n",
    "        replace('%', ' percent ').\n",
    "        replace('):', ' ').\n",
    "        replace('.gov', ' '))\n",
    "\n",
    "        for word in alt_stop_words:\n",
    "\n",
    "            cleaned = (cleaned.replace(word, ''))\n",
    "\n",
    "        text = re.sub(r'\\.\\.+', '', cleaned)\n",
    "        text = re.sub(r'\\.', '', cleaned)\n",
    "        text = re.sub('[()]', '', text)\n",
    "\n",
    "        text = ''.join(x for x in text if x in string.printable)\n",
    "        text = ''.join([i for i in text if not i.isdigit()])\n",
    "        text = ''.join([i for i in text if text not in string.punctuation])\n",
    "\n",
    "        self.abbreviations += (re.findall(r\"\\b[A-Z]{3}\\b\", text))\n",
    "\n",
    "        return text\n",
    "    \n",
    "    def clean_articles(self):\n",
    "        \n",
    "        for i,article in enumerate(self.articles):\n",
    "            \n",
    "            self.articles[i] = self.doc_cleaner(doc = article['text'])\n",
    "            \n",
    "        self.abbreviations = set(self.abbreviations)\n",
    "        \n",
    "        for i,article in enumerate(self.articles):\n",
    "            \n",
    "            listed_abb = list(self.abbreviations)\n",
    "            \n",
    "            for word in listed_abb:\n",
    "            \n",
    "                self.articles[i] = re.sub(word, '',article)\n",
    "            \n",
    "            \n",
    "    def add_stopwords(self, stop_words):\n",
    "        \n",
    "        self.user_det_stop_words += stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cl = cleaner(articles = [db.raw_pdfs.find({})[i][unique] for i,unique in enumerate(uniques)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cl.clean_articles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, article in enumerate(uniques):\n",
    "    \n",
    "    db.cleaned_pdfs.insert_one({article: cl.articles[i]})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py3.6]",
   "language": "python",
   "name": "Python [py3.6]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
